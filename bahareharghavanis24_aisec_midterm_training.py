# -*- coding: utf-8 -*-
"""BaharehArghavaniS24-AISec-Midterm-Training.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1XRsNkHE4-2ojvCGAIb1rxPhyTzHL6LTu

**Revised on 3/5/2024: Changed source files**

This is the skeleton code for Task 1 of the midterm project. The files that are downloaded in step 4 are based on the [Ember 2018 dataset](https://arxiv.org/abs/1804.04637), and contain the features (and corresponding labels) extracted from 1 million PE files, split into 80\% training and 20\% test datasets. The code used for for feature extraction is available [here](https://colab.research.google.com/drive/16q9bOlCmnTquPtVXVzxUj4ZY1ORp10R2?usp=sharing). However, the preprocessing and featurization process may take up to 3 hours on Google Colab. Hence, I recommend using the processed datasets (Step 4) to speed up your development.

Also, note that there is a new optional step 8.5 - To speed up your experiments, you may want to sample the original dataset of 800k training samples and 200k test samples to smaller datasets.

**Step 1:** Mount your Google Drive by clicking on "Mount Drive" in the Files section (panel to the left of this text.)

**Step 2:** Go to Runtime -> Change runtime type and select T4 GPU.

**Step 3:** Create a folder in your Google Drive, and rename it to "vMalConv"

**Step 4:** Download the pre-processed training and test datasets.
"""

# ~8GB
!wget https://dsci6015s24-midterm.s3.amazonaws.com/v2/X_train.dat
!wget https://dsci6015s24-midterm.s3.amazonaws.com/v2/X_test.dat
!wget https://dsci6015s24-midterm.s3.amazonaws.com/v2/y_train.dat
!wget https://dsci6015s24-midterm.s3.amazonaws.com/v2/y_test.dat
!wget https://dsci6015s24-midterm.s3.amazonaws.com/v2/metadata.csv

from google.colab import drive
drive.mount('/content/drive')

"""**Step 5:** Copy the downloaded files to vMalConv"""

!cp /content/X_train.dat /content/drive/MyDrive/vMalConv/X_train.dat
!cp /content/X_test.dat /content/drive/MyDrive/vMalConv/X_test.dat
!cp /content/y_train.dat /content/drive/MyDrive/vMalConv/y_train.dat
!cp /content/y_test.dat /content/drive/MyDrive/vMalConv/y_test.dat
!cp /content/metadata.csv /content/drive/MyDrive/vMalConv/metadata.csv

"""**Step 6:** Download and install Ember:"""

!pip install git+https://github.com/PFGimenez/ember.git

!pip install lief

"""**Step 7:** Read vectorized features from the data files."""

import ember
X_train, y_train, X_test, y_test = ember.read_vectorized_features("drive/MyDrive/vMalConv/")
metadata_dataframe = ember.read_metadata("drive/MyDrive/vMalConv/")

"""**Step 8:** Get rid of rows with no labels."""

metadata_dataframe

labelrows = (y_train != -1)
X_train = X_train[labelrows]
y_train = y_train[labelrows]

import h5py
h5f = h5py.File('X_train.h5', 'w')
h5f.create_dataset('X_train', data=X_train)
h5f.close()
h5f = h5py.File('y_train.h5', 'w')
h5f.create_dataset('y_train', data=y_train)
h5f.close()

!cp /content/X_train.h5 /content/drive/MyDrive/vMalConv/X_train.h5
!cp /content/y_train.h5 /content/drive/MyDrive/vMalConv/y_train.h5

print("X_train.shape:",X_train.shape)
print("y_train.shape:",y_train.shape)
print("X_test.shape:",X_test.shape)
print("y_test.shape:",y_test.shape)

print("X_train.dtype:",X_train.dtype)
print("y_train.dtype:",y_train.dtype)

"""**Optional Step 8.5:** To speed up your experiments, you may want to sample the original dataset of 800k training samples and 200k test samples to smaller datasets. You can use the [Pandas Dataframe sample() method](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.sample.html), or come up with your own sampling methodology. Be mindful of the fact that the database is heavily imbalanced."""

### Your code (optional) for sampling the original dataset.

"""> **Task 1:** Complete the following code to build the architecture of MalConv in PyTorch:"""

#Bibek
import torch
import torch.nn as nn
import torch.nn.functional as F

class MalConv(nn.Module):
    def __init__(self, input_length=2000000, embedding_dim=8, window_size=500, output_dim=1):
        super(MalConv, self).__init__()
        self.embed = nn.Embedding(2381, embedding_dim)  # Assuming 2381 as the input_dim
        self.conv1 = nn.Conv1d(embedding_dim, 128, kernel_size=window_size, stride=window_size, padding=0)
        self.conv2 = nn.Conv1d(embedding_dim, 128, kernel_size=window_size, stride=window_size, padding=0)
        self.gating = nn.Sigmoid()
        self.global_max_pool = nn.AdaptiveMaxPool1d(1)
        self.fc1 = nn.Linear(128, 128)
        self.fc2 = nn.Linear(128, output_dim)
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        x = self.embed(x)
        x = x.transpose(1, 2)  # Convert to (batch_size, channels, length)
        conv1 = self.conv1(x)
        conv2 = self.conv2(x)
        gated = conv1 * self.gating(conv2)  # Element-wise multiplication
        global_max_pool = self.global_max_pool(gated).squeeze(2)  # Remove the last dimension
        fc1 = F.relu(self.fc1(global_max_pool))
        fc2 = self.fc2(fc1)
        output = self.sigmoid(fc2)
        return output

# Example of creating and testing a MalConv model
input_length = 2000000  # The fixed length for each input file
model = MalConv(input_length=input_length)
print(model)

# Assuming the use of preprocessed byte sequences tensor, below is a placeholder
example_input = torch.randint(0, 2381, (4, input_length), dtype=torch.long)  # 4 examples, simulated data
output = model(example_input)
print(output.shape)  # The output probabilities for each example

"""**Step 8:** Partial fit the standardScaler to avoid overloading the memory:"""

# Feature Engineering
#we want to standardize the feature, we can use MinMaxScaler like we have used in previous calsses
import sklearn
ss= sklearn.preprocessing.MinMaxScaler()
X_train=ss.fit_transform(X_train)
X_test=ss.fit_transform(X_test)

X_train.shape

X_train

"""No Need"""

## Reshape to create 3 channels ##
#import numpy as np
#X_train = np.reshape(X_train,(-1,1,2381))
#y_train = np.reshape(y_train,(-1,1,1))

"""**Load, Tensorize, and Split** The following code takes care of converting the training data into Torch Tensors, and then splits it into 80% training and 20% validation datasets."""

print(X_train.dtype)
print(y_train.dtype)

import numpy as np
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
from sklearn.model_selection import train_test_split

# Assuming MalConv class definition is already provided as above

# Convert your numpy arrays to PyTorch tensors
X_train_tensor = torch.tensor(X_train, dtype=torch.long)
y_train_tensor = torch.tensor(y_train, dtype=torch.float32)

# Split the data into training and validation sets (80% training, 20% validation)
X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(
    X_train_tensor, y_train_tensor, test_size=0.2, random_state=42
)

# Create TensorDatasets and DataLoaders for training and validation sets
train_dataset = TensorDataset(X_train_split, y_train_split)
val_dataset = TensorDataset(X_val_split, y_val_split)

batch_size = 2048  # Adjust based on your GPU memory
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)

"""> **Task 2:** Complete the following code to train the model on the GPU for 15 epochs, with a batch size of 64. If you are on Google Colab, don't forget to change the kernel in Runtime -> Change runtime type -> T4 GPU."""

#Bibek
import torch
import torch.optim as optim
import torch.nn.functional as F
import os
import matplotlib.pyplot as plt

# Assuming MalConv is defined as before
model = MalConv()

# Move model to GPU if available
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

# Loss function and optimizer
criterion = nn.BCELoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

# Directory to save model checkpoints
save_dir = "./model_checkpoints"
os.makedirs(save_dir, exist_ok=True)

# Training Loop with Validation
num_epochs = 30
train_losses = []
val_losses = []

for epoch in range(num_epochs):
    model.train()
    running_loss = 0.0

    for inputs, labels in train_loader:
        inputs, labels = inputs.to(device), labels.to(device)

        optimizer.zero_grad()

        outputs = model(inputs)
        loss = criterion(outputs.squeeze(), labels.float())
        loss.backward()
        optimizer.step()

        running_loss += loss.item()

    epoch_loss = running_loss / len(train_loader)
    train_losses.append(epoch_loss)

    model.eval()
    val_loss = 0.0
    with torch.no_grad():
        for inputs, labels in val_loader:
            inputs, labels = inputs.to(device), labels.to(device)
            outputs = model(inputs)
            loss = criterion(outputs.squeeze(), labels.float())
            val_loss += loss.item()

    epoch_val_loss = val_loss / len(val_loader)
    val_losses.append(epoch_val_loss)

    print(f'Epoch {epoch+1}, Training Loss: {epoch_loss}, Validation Loss: {epoch_val_loss}')

    if (epoch + 1) % 5 == 0:
        checkpoint_path = os.path.join(save_dir, f'model_epoch_{epoch+1}.pt')
        torch.save({
            'epoch': epoch + 1,
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'loss': loss,
        }, checkpoint_path)
        print(f'Model checkpoint saved to {checkpoint_path}')

# Plotting the training and validation loss
plt.figure(figsize=(10, 5))
plt.plot(train_losses, label='Training Loss')
plt.plot(val_losses, label='Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.title('Training and Validation Loss')
plt.legend()
plt.show()

from sklearn.metrics import accuracy_score, precision_score, recall_score

# Convert test data to PyTorch tensors
X_test_tensor = torch.tensor(X_test, dtype=torch.long)
y_test_tensor = torch.tensor(y_test, dtype=torch.float32)

# Create a TensorDataset and DataLoader for test data
test_dataset = TensorDataset(X_test_tensor, y_test_tensor)
test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

# Ensure the model is in evaluation mode
model.eval()

# Lists to store model predictions and actual labels
predictions = []
labels_list = []  # Renamed to avoid confusion with the loop variable

with torch.no_grad():
    for inputs, labels_batch in test_loader:
        inputs = inputs.to(device)  # Move inputs to the same device as model
        outputs = model(inputs)
        predicted = torch.round(outputs.squeeze())  # Convert probabilities to binary predictions (0 or 1)

        # Store predictions and labels
        predictions.extend(predicted.cpu().numpy())
        labels_list.extend(labels_batch.cpu().numpy())

# Compute metrics
accuracy = accuracy_score(labels_list, predictions)
precision = precision_score(labels_list, predictions)
recall = recall_score(labels_list, predictions)

print(f'Test Accuracy: {accuracy:.4f}')
print(f'Precision: {precision:.4f}')
print(f'Recall: {recall:.4f}')

# Save the model parameters
model_weights_path = "malConv_model_weights.pt"
torch.save(model.state_dict(), model_weights_path)
print(f"Model weights saved to {model_weights_path}")
# Save a checkpoint including model state, optimizer state, and other info
checkpoint_path = os.path.join(save_dir, "model_final_checkpoint.pt")
torch.save({
    'epoch': num_epochs,
    'model_state_dict': model.state_dict(),
    'optimizer_state_dict': optimizer.state_dict(),
    'last_loss': loss.item(),
    # Include any other information you need
}, checkpoint_path)
print(f"Full model checkpoint saved to {checkpoint_path}")

#reload model
# Assuming the MalConv class is defined in the script

# Create a new model instance
model = MalConv()

# Load the weights
model.load_state_dict(torch.load("malConv_model_weights.pt"))

# If you're continuing training or need the optimizer state, load the full checkpoint
checkpoint = torch.load("/content/model_checkpoints/model_final_checkpoint.pt")
model.load_state_dict(checkpoint['model_state_dict'])
optimizer = optim.Adam(model.parameters(), lr=0.001)  # Adjust hyperparameters as needed
optimizer.load_state_dict(checkpoint['optimizer_state_dict'])

# Now the model and optimizer are ready to be used

import torch
# Assuming MalConv is defined in the same script or imported from another module
model = MalConv()

# Load the model checkpoint
checkpoint_path = "drive/MyDrive/vMalConv/model_epoch_10.pt" # Adjust the path and filename as needed
model.load_state_dict(torch.load(checkpoint_path))

# Move model to GPU if available
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

# Set the model to evaluation mode
model.eval()

print("Loaded model from disk")

"""**Task 4:** Comment on the results in this text box."""