{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61eeeafd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (2.2.1)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch) (4.9.0)\n",
      "Requirement already satisfied: sympy in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch) (2023.12.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch) (2.19.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: triton==2.2.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch) (2.2.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.4.99)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from jinja2->torch) (2.1.4)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "716c78ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/PFGimenez/ember.git\n",
      "  Cloning https://github.com/PFGimenez/ember.git to /tmp/pip-req-build-u6al8krl\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/PFGimenez/ember.git /tmp/pip-req-build-u6al8krl\n",
      "  Resolved https://github.com/PFGimenez/ember.git to commit 3b82fe63069884882e743af725d29cc2a67859f1\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/PFGimenez/ember.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c529be1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lightgbm in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (4.3.0)\n",
      "Requirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from lightgbm) (1.22.4)\n",
      "Requirement already satisfied: scipy in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from lightgbm) (1.12.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "057e9a04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lief==0.12.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (0.12.3)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install lief==0.12.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b678504d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import ember"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fac5fe89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-03-12 16:22:56--  https://github.com/iosifache/DikeDataset/blob/main/files/benign/002ce0d28ec990aadbbc89df457189de37d8adaadc9c084b78eb7be9a9820c81.exe\n",
      "Resolving github.com (github.com)... 140.82.113.4\n",
      "Connecting to github.com (github.com)|140.82.113.4|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 195972 (191K) [text/plain]\n",
      "Saving to: ‘002ce0d28ec990aadbbc89df457189de37d8adaadc9c084b78eb7be9a9820c81.exe’\n",
      "\n",
      "100%[======================================>] 195,972     --.-K/s   in 0.007s  \n",
      "\n",
      "2024-03-12 16:22:56 (27.8 MB/s) - ‘002ce0d28ec990aadbbc89df457189de37d8adaadc9c084b78eb7be9a9820c81.exe’ saved [195972/195972]\n",
      "\n",
      "--2024-03-12 16:22:56--  https://github.com/iosifache/DikeDataset/blob/main/files/benign/02431aa60089b968bc59acc69796ed9418546894752d0c9766fbe3aae0a85031.exe\n",
      "Resolving github.com (github.com)... 140.82.113.4\n",
      "Connecting to github.com (github.com)|140.82.113.4|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 195906 (191K) [text/plain]\n",
      "Saving to: ‘02431aa60089b968bc59acc69796ed9418546894752d0c9766fbe3aae0a85031.exe’\n",
      "\n",
      "100%[======================================>] 195,906     --.-K/s   in 0.006s  \n",
      "\n",
      "2024-03-12 16:22:57 (30.7 MB/s) - ‘02431aa60089b968bc59acc69796ed9418546894752d0c9766fbe3aae0a85031.exe’ saved [195906/195906]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://github.com/iosifache/DikeDataset/blob/main/files/benign/002ce0d28ec990aadbbc89df457189de37d8adaadc9c084b78eb7be9a9820c81.exe\n",
    "\n",
    "!wget https://github.com/iosifache/DikeDataset/blob/main/files/benign/02431aa60089b968bc59acc69796ed9418546894752d0c9766fbe3aae0a85031.exe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0e7a12ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MalConv(nn.Module):\n",
    "    def __init__(self, input_length=2000000, embedding_dim=8, window_size=500, output_dim=1):\n",
    "        super(MalConv, self).__init__()\n",
    "        self.embed = nn.Embedding(2381, embedding_dim)  # Assuming 2381 as the input_dim\n",
    "        self.conv1 = nn.Conv1d(embedding_dim, 128, kernel_size=window_size, stride=window_size, padding=0)\n",
    "        self.conv2 = nn.Conv1d(embedding_dim, 128, kernel_size=window_size, stride=window_size, padding=0)\n",
    "        self.gating = nn.Sigmoid()\n",
    "        self.global_max_pool = nn.AdaptiveMaxPool1d(1)\n",
    "        self.fc1 = nn.Linear(128, 128)\n",
    "        self.fc2 = nn.Linear(128, output_dim)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embed(x)\n",
    "        x = x.transpose(1, 2)  # Convert to (batch_size, channels, length)\n",
    "        conv1 = self.conv1(x)\n",
    "        conv2 = self.conv2(x)\n",
    "        gated = conv1 * self.gating(conv2)  # Element-wise multiplication\n",
    "        global_max_pool = self.global_max_pool(gated).squeeze(2)  # Remove the last dimension\n",
    "        fc1 = F.relu(self.fc1(global_max_pool))\n",
    "        fc2 = self.fc2(fc1)\n",
    "        output = self.sigmoid(fc2)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "92aef854",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to load the trained model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def load_model(model_path):\n",
    "    # Initialize the model\n",
    "    model = MalConv()  \n",
    "\n",
    "    # Load the model weights from the checkpoint\n",
    "    checkpoint = torch.load(model_path, map_location=torch.device('cpu'))  # Load the checkpoint\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])  # Load the model state dictionary\n",
    "    \n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a56ecc1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MalConv(\n",
       "  (embed): Embedding(2381, 8)\n",
       "  (conv1): Conv1d(8, 128, kernel_size=(500,), stride=(500,))\n",
       "  (conv2): Conv1d(8, 128, kernel_size=(500,), stride=(500,))\n",
       "  (gating): Sigmoid()\n",
       "  (global_max_pool): AdaptiveMaxPool1d(output_size=1)\n",
       "  (fc1): Linear(in_features=128, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=1, bias=True)\n",
       "  (sigmoid): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_model(\"vMalConv/model_epoch_15.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3da8d216",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to preprocess the PE file\n",
    "def preprocess_PE_file(file_path, input_length):\n",
    "    \n",
    "    test_file = open(file_path, 'rb').read() # read PE file\n",
    "    extract_PE = ember.PEFeatureExtractor() # initialize extractor\n",
    "    input_PE = extract_PE.feature_vector(test_file) # read feature vector\n",
    "    input_tensor = torch.tensor(input_PE)\n",
    "    byte_seq = input_tensor.byte() # get byte sequence\n",
    "    input_data = byte_seq.reshape(1, 2381) \n",
    "    \n",
    "    return input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "40303b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to classify PE file using the trained model\n",
    "def classify_PE_file(file_path, model_path, input_length):\n",
    "    # Preprocess the PE file\n",
    "    input_data = preprocess_PE_file(file_path, input_length)\n",
    "\n",
    "    # Run the preprocessed data through the trained model\n",
    "    model = load_model(model_path)  # load model\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    \n",
    "    # Convert input_data tensor to type Long\n",
    "    input_data = input_data.long()  # or input_data.int() depending on the input type\n",
    "    \n",
    "    input_data = input_data.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model(input_data)\n",
    "\n",
    "    prediction = output > 0.5\n",
    "\n",
    "    if prediction:\n",
    "        return \"Malware\"\n",
    "    else:\n",
    "        return \"Benign\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d71705d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.12.3-39115d10\n"
     ]
    }
   ],
   "source": [
    "import lief\n",
    "print(lief.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b6f772fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: EMBER feature version 2 were computed using lief version 0.9.0-\n",
      "WARNING:   lief version 0.12.3-39115d10 found instead. There may be slight inconsistencies\n",
      "WARNING:   in the feature calculations.\n",
      "Prediction: Benign\n"
     ]
    }
   ],
   "source": [
    "model_path = 'vMalConv/model_epoch_15.pt'\n",
    "file_b1 = '002ce0d28ec990aadbbc89df457189de37d8adaadc9c084b78eb7be9a9820c81.exe'\n",
    "file_b2 = '02431aa60089b968bc59acc69796ed9418546894752d0c9766fbe3aae0a85031.exe'\n",
    "label = 0\n",
    "input_length = 2000000\n",
    "\n",
    "# Classify the PE file\n",
    "prediction = classify_PE_file(file_b1, model_path, input_length)\n",
    "\n",
    "# Print the prediction\n",
    "print(\"Prediction:\", prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba8bf62",
   "metadata": {},
   "source": [
    "Upload model to S3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "369792f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "s3 = boto3.resource(\"s3\")\n",
    "\n",
    "s3.Bucket(\"malwares3\").upload_file(model_path, 'model_epoch_15.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bf18dd85",
   "metadata": {},
   "outputs": [
    {
     "ename": "ClientError",
     "evalue": "An error occurred (ValidationException) when calling the CreateEndpointConfig operation: 1 validation error detected: Value 'ml.t3.medium' at 'productionVariants.1.member.instanceType' failed to satisfy constraint: Member must satisfy enum value set: [ml.r7i.48xlarge, ml.trn1.32xlarge, ml.r5d.12xlarge, ml.r5.12xlarge, ml.p2.xlarge, ml.m5.4xlarge, ml.m4.16xlarge, ml.r7i.16xlarge, ml.m7i.xlarge, ml.p5.48xlarge, ml.r6gd.xlarge, ml.r6g.8xlarge, ml.r6g.large, ml.m6gd.16xlarge, ml.r5d.24xlarge, ml.r5.24xlarge, ml.r7i.8xlarge, ml.r7i.large, ml.m7i.12xlarge, ml.r6gd.12xlarge, ml.r6g.16xlarge, ml.m6gd.8xlarge, ml.m6gd.large, ml.m6g.xlarge, ml.p4d.24xlarge, ml.m7i.24xlarge, ml.m6g.12xlarge, ml.g5.2xlarge, ml.p3.16xlarge, ml.m5d.xlarge, ml.m5.large, ml.t2.xlarge, ml.m7i.48xlarge, ml.p2.16xlarge, ml.m5d.12xlarge, ml.m7i.16xlarge, ml.r6gd.16xlarge, ml.c6gd.2xlarge, ml.g5.4xlarge, ml.inf1.2xlarge, ml.m5d.24xlarge, ml.m6g.16xlarge, ml.c4.2xlarge, ml.c6gn.xlarge, ml.c6gd.4xlarge, ml.c5.2xlarge, ml.c6gn.12xlarge, ml.c6i.32xlarge, ml.c4.4xlarge, ml.g5.8xlarge, ml.c6i.xlarge, ml.inf1.6xlarge, ml.c5d.2xlarge, ml.c5.4xlarge, ml.c7i.xlarge, ml.c7g.2xlarge, ml.c6i.12xlarge, ml.g4dn.xlarge, ml.c7i.12xlarge, ml.c6gd.8xlarge, ml.c6gd.large, ml.c6g.2xlarge, ml.c6g.xlarge, ml.c6i.24xlarge, ml.g4dn.12xlarge, ml.c5d.4xlarge, ml.c7i.24xlarge, ml.c7i.2xlarge, ml.inf2.8xlarge, ml.c6gn.16xlarge, ml.c6g.12xlarge, ml.c7g.4xlarge, ml.c7g.xlarge, ml.g4dn.2xlarge, ml.c4.8xlarge, ml.c4.large, ml.c6g.4xlarge, ml.c7g.12xlarge, ml.c6i.2xlarge, ml.c5d.xlarge, ml.c5.large, ml.c7i.48xlarge, ml.c7i.4xlarge, ml.c6i.16xlarge, ml.g4dn.4xlarge, ml.c5.9xlarge, ml.c7i.16xlarge, ml.c6gn.2xlarge, ml.c6i.4xlarge, ml.g4dn.16xlarge, ml.c5d.large, ml.c5.xlarge, ml.inf2.xlarge, ml.c6g.16xlarge, ml.c7g.8xlarge, ml.c7g.large, ml.c5d.9xlarge, ml.c4.xlarge, ml.trn1n.32xlarge, ml.c6gn.4xlarge, ml.c6gd.xlarge, ml.c6g.8xlarge, ml.c6g.large, ml.c7g.16xlarge, ml.inf1.xlarge, ml.c7i.8xlarge, ml.c7i.large, ml.inf2.24xlarge, ml.c6gd.12xlarge, ml.g4dn.8xlarge, ml.g5.xlarge, ml.c6i.8xlarge, ml.c6i.large, ml.inf1.24xlarge, ml.m5d.2xlarge, ml.t2.2xlarge, ml.inf2.48xlarge, ml.g5.12xlarge, ml.c5d.18xlarge, ml.c6gn.8xlarge, ml.c6gn.large, ml.m6g.2xlarge, ml.g5.24xlarge, ml.m5d.4xlarge, ml.t2.medium, ml.m7i.2xlarge, ml.trn1.2xlarge, ml.r6gd.2xlarge, ml.c6gd.16xlarge, ml.c5.18xlarge, ml.m6g.4xlarge, ml.g5.48xlarge, ml.m7i.4xlarge, ml.r6gd.4xlarge, ml.g5.16xlarge, ml.dl1.24xlarge, ml.r5d.2xlarge, ml.r5.2xlarge, ml.p3.2xlarge, ml.m5d.large, ml.m5.xlarge, ml.m4.10xlarge, ml.t2.large, ml.r6g.2xlarge, ml.r5d.4xlarge, ml.r5.4xlarge, ml.m5.12xlarge, ml.m4.xlarge, ml.r7i.2xlarge, ml.r7i.xlarge, ml.m6gd.2xlarge, ml.m6gd.xlarge, ml.m6g.8xlarge, ml.m6g.large, ml.m5.24xlarge, ml.r7i.12xlarge, ml.m7i.8xlarge, ml.m7i.large, ml.r6gd.8xlarge, ml.r6gd.large, ml.r6g.4xlarge, ml.r6g.xlarge, ml.m6gd.12xlarge, ml.m4.2xlarge, ml.r7i.24xlarge, ml.r7i.4xlarge, ml.r6g.12xlarge, ml.m6gd.4xlarge, ml.p2.8xlarge, ml.m5.2xlarge, ml.p4de.24xlarge, ml.r5d.xlarge, ml.r5d.large, ml.r5.xlarge, ml.r5.large, ml.p3.8xlarge, ml.m4.4xlarge]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mClientError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 20\u001b[0m\n\u001b[1;32m     13\u001b[0m model \u001b[38;5;241m=\u001b[39m PyTorchModel(model_data\u001b[38;5;241m=\u001b[39mmodel_data,\n\u001b[1;32m     14\u001b[0m                       role\u001b[38;5;241m=\u001b[39mrole,\n\u001b[1;32m     15\u001b[0m                       entry_point\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minference.py\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     16\u001b[0m                      source_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel/code/\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     17\u001b[0m                       py_version\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpy3\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Deploy the model to an endpoint\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m predictor \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeploy\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstance_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mml.t3.medium\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial_instance_count\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Use the endpoint for inference\u001b[39;00m\n\u001b[1;32m     23\u001b[0m result \u001b[38;5;241m=\u001b[39m predictor\u001b[38;5;241m.\u001b[39mpredict(input_data)\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/sagemaker/model.py:1654\u001b[0m, in \u001b[0;36mModel.deploy\u001b[0;34m(self, initial_instance_count, instance_type, serializer, deserializer, accelerator_type, endpoint_name, tags, kms_key, wait, data_capture_config, async_inference_config, serverless_inference_config, volume_size, model_data_download_timeout, container_startup_health_check_timeout, inference_recommendation_id, explainer_config, accept_eula, endpoint_logging, resources, endpoint_type, managed_instance_scaling, **kwargs)\u001b[0m\n\u001b[1;32m   1651\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_explainer_enabled:\n\u001b[1;32m   1652\u001b[0m     explainer_config_dict \u001b[38;5;241m=\u001b[39m explainer_config\u001b[38;5;241m.\u001b[39m_to_request_dict()\n\u001b[0;32m-> 1654\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msagemaker_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mendpoint_from_production_variants\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1655\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mendpoint_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1656\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproduction_variants\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mproduction_variant\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1657\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1658\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkms_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkms_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1659\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwait\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1660\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_capture_config_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_capture_config_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1661\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexplainer_config_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexplainer_config_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1662\u001b[0m \u001b[43m    \u001b[49m\u001b[43masync_inference_config_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43masync_inference_config_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1663\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlive_logging\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint_logging\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1664\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1666\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor_cls:\n\u001b[1;32m   1667\u001b[0m     predictor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor_cls(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mendpoint_name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msagemaker_session)\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/sagemaker/session.py:5435\u001b[0m, in \u001b[0;36mSession.endpoint_from_production_variants\u001b[0;34m(self, name, production_variants, tags, kms_key, wait, data_capture_config_dict, async_inference_config_dict, explainer_config_dict, live_logging, vpc_config, enable_network_isolation, role)\u001b[0m\n\u001b[1;32m   5432\u001b[0m     config_options[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExecutionRoleArn\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m role\n\u001b[1;32m   5434\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating endpoint-config with name \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, name)\n\u001b[0;32m-> 5435\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msagemaker_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_endpoint_config\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mconfig_options\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5437\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_endpoint(\n\u001b[1;32m   5438\u001b[0m     endpoint_name\u001b[38;5;241m=\u001b[39mname,\n\u001b[1;32m   5439\u001b[0m     config_name\u001b[38;5;241m=\u001b[39mname,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5442\u001b[0m     live_logging\u001b[38;5;241m=\u001b[39mlive_logging,\n\u001b[1;32m   5443\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/botocore/client.py:553\u001b[0m, in \u001b[0;36mClientCreator._create_api_method.<locals>._api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    549\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    550\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpy_operation_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m() only accepts keyword arguments.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    551\u001b[0m     )\n\u001b[1;32m    552\u001b[0m \u001b[38;5;66;03m# The \"self\" in this scope is referring to the BaseClient.\u001b[39;00m\n\u001b[0;32m--> 553\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_api_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43moperation_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/botocore/client.py:1009\u001b[0m, in \u001b[0;36mBaseClient._make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m   1005\u001b[0m     error_code \u001b[38;5;241m=\u001b[39m error_info\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQueryErrorCode\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m error_info\u001b[38;5;241m.\u001b[39mget(\n\u001b[1;32m   1006\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCode\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1007\u001b[0m     )\n\u001b[1;32m   1008\u001b[0m     error_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mfrom_code(error_code)\n\u001b[0;32m-> 1009\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error_class(parsed_response, operation_name)\n\u001b[1;32m   1010\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1011\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parsed_response\n",
      "\u001b[0;31mClientError\u001b[0m: An error occurred (ValidationException) when calling the CreateEndpointConfig operation: 1 validation error detected: Value 'ml.t3.medium' at 'productionVariants.1.member.instanceType' failed to satisfy constraint: Member must satisfy enum value set: [ml.r7i.48xlarge, ml.trn1.32xlarge, ml.r5d.12xlarge, ml.r5.12xlarge, ml.p2.xlarge, ml.m5.4xlarge, ml.m4.16xlarge, ml.r7i.16xlarge, ml.m7i.xlarge, ml.p5.48xlarge, ml.r6gd.xlarge, ml.r6g.8xlarge, ml.r6g.large, ml.m6gd.16xlarge, ml.r5d.24xlarge, ml.r5.24xlarge, ml.r7i.8xlarge, ml.r7i.large, ml.m7i.12xlarge, ml.r6gd.12xlarge, ml.r6g.16xlarge, ml.m6gd.8xlarge, ml.m6gd.large, ml.m6g.xlarge, ml.p4d.24xlarge, ml.m7i.24xlarge, ml.m6g.12xlarge, ml.g5.2xlarge, ml.p3.16xlarge, ml.m5d.xlarge, ml.m5.large, ml.t2.xlarge, ml.m7i.48xlarge, ml.p2.16xlarge, ml.m5d.12xlarge, ml.m7i.16xlarge, ml.r6gd.16xlarge, ml.c6gd.2xlarge, ml.g5.4xlarge, ml.inf1.2xlarge, ml.m5d.24xlarge, ml.m6g.16xlarge, ml.c4.2xlarge, ml.c6gn.xlarge, ml.c6gd.4xlarge, ml.c5.2xlarge, ml.c6gn.12xlarge, ml.c6i.32xlarge, ml.c4.4xlarge, ml.g5.8xlarge, ml.c6i.xlarge, ml.inf1.6xlarge, ml.c5d.2xlarge, ml.c5.4xlarge, ml.c7i.xlarge, ml.c7g.2xlarge, ml.c6i.12xlarge, ml.g4dn.xlarge, ml.c7i.12xlarge, ml.c6gd.8xlarge, ml.c6gd.large, ml.c6g.2xlarge, ml.c6g.xlarge, ml.c6i.24xlarge, ml.g4dn.12xlarge, ml.c5d.4xlarge, ml.c7i.24xlarge, ml.c7i.2xlarge, ml.inf2.8xlarge, ml.c6gn.16xlarge, ml.c6g.12xlarge, ml.c7g.4xlarge, ml.c7g.xlarge, ml.g4dn.2xlarge, ml.c4.8xlarge, ml.c4.large, ml.c6g.4xlarge, ml.c7g.12xlarge, ml.c6i.2xlarge, ml.c5d.xlarge, ml.c5.large, ml.c7i.48xlarge, ml.c7i.4xlarge, ml.c6i.16xlarge, ml.g4dn.4xlarge, ml.c5.9xlarge, ml.c7i.16xlarge, ml.c6gn.2xlarge, ml.c6i.4xlarge, ml.g4dn.16xlarge, ml.c5d.large, ml.c5.xlarge, ml.inf2.xlarge, ml.c6g.16xlarge, ml.c7g.8xlarge, ml.c7g.large, ml.c5d.9xlarge, ml.c4.xlarge, ml.trn1n.32xlarge, ml.c6gn.4xlarge, ml.c6gd.xlarge, ml.c6g.8xlarge, ml.c6g.large, ml.c7g.16xlarge, ml.inf1.xlarge, ml.c7i.8xlarge, ml.c7i.large, ml.inf2.24xlarge, ml.c6gd.12xlarge, ml.g4dn.8xlarge, ml.g5.xlarge, ml.c6i.8xlarge, ml.c6i.large, ml.inf1.24xlarge, ml.m5d.2xlarge, ml.t2.2xlarge, ml.inf2.48xlarge, ml.g5.12xlarge, ml.c5d.18xlarge, ml.c6gn.8xlarge, ml.c6gn.large, ml.m6g.2xlarge, ml.g5.24xlarge, ml.m5d.4xlarge, ml.t2.medium, ml.m7i.2xlarge, ml.trn1.2xlarge, ml.r6gd.2xlarge, ml.c6gd.16xlarge, ml.c5.18xlarge, ml.m6g.4xlarge, ml.g5.48xlarge, ml.m7i.4xlarge, ml.r6gd.4xlarge, ml.g5.16xlarge, ml.dl1.24xlarge, ml.r5d.2xlarge, ml.r5.2xlarge, ml.p3.2xlarge, ml.m5d.large, ml.m5.xlarge, ml.m4.10xlarge, ml.t2.large, ml.r6g.2xlarge, ml.r5d.4xlarge, ml.r5.4xlarge, ml.m5.12xlarge, ml.m4.xlarge, ml.r7i.2xlarge, ml.r7i.xlarge, ml.m6gd.2xlarge, ml.m6gd.xlarge, ml.m6g.8xlarge, ml.m6g.large, ml.m5.24xlarge, ml.r7i.12xlarge, ml.m7i.8xlarge, ml.m7i.large, ml.r6gd.8xlarge, ml.r6gd.large, ml.r6g.4xlarge, ml.r6g.xlarge, ml.m6gd.12xlarge, ml.m4.2xlarge, ml.r7i.24xlarge, ml.r7i.4xlarge, ml.r6g.12xlarge, ml.m6gd.4xlarge, ml.p2.8xlarge, ml.m5.2xlarge, ml.p4de.24xlarge, ml.r5d.xlarge, ml.r5d.large, ml.r5.xlarge, ml.r5.large, ml.p3.8xlarge, ml.m4.4xlarge]"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "from sagemaker.pytorch import PyTorchModel\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "role = get_execution_role()\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "# Specify your model location in S3\n",
    "model_data = 's3://malwares3/model.tar.gz'\n",
    "\n",
    "# Create a SageMaker model\n",
    "model = PyTorchModel(model_data=model_data,\n",
    "                      role=role,\n",
    "                      entry_point='inference.py',\n",
    "                     source_dir=\"model/code/\",\n",
    "                      py_version='py3')\n",
    "\n",
    "# Deploy the model to an endpoint\n",
    "predictor = model.deploy(instance_type='ml.r5.xlarge', initial_instance_count=1)\n",
    "\n",
    "# Use the endpoint for inference\n",
    "result = predictor.predict(input_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f59b7b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
